{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ob5fTzLcJSnj"
   },
   "source": [
    "# NLP (Natural Language Processing) Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Exploratory Data Analysis (Frequency Distribution | Parse Trees)\n",
    "2. Text Preprocessing (Tokenize, Stem, Lemmatize, Vectorize)\n",
    "3. Feature Engineering (Bigrams, POS-Tags, TF-IDF)\n",
    "4. Modeling\n",
    "5. Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pa8jvj1LLEL9"
   },
   "source": [
    "## EDA (Exploratory Data Analysis)\n",
    "\n",
    "\"Exploratory Data Analysis is the process of exploring data, generating insights, testing hypotheses, checking assumptions and revealing underlying hidden patterns in the data\".\n",
    "\n",
    "Through these we can get a basic description of the data, visualize it, identify pattern in it, identify potential challenges of using the data, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OAiKbrhvTvBD"
   },
   "source": [
    "The dataset (zip file) can be downloaded from this URL -- https://archive.ics.uci.edu/ml/machine-learning-databases/00228/. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ExCELPIgLXet"
   },
   "outputs": [],
   "source": [
    "#import pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DlCiEGN0Lddx"
   },
   "outputs": [],
   "source": [
    "sms = pd.read_table('./SMSSpamCollection', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "654CaLnWL5Jz",
    "outputId": "b438d2fd-7a3b-4118-add4-588c3ad52e3e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0                                                  1\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see the top 5 records\n",
    "sms.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zEDEHD0oJXzD"
   },
   "source": [
    "### Describe the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q7GI-NSJMypB"
   },
   "source": [
    "To begin, we can use the describe() function to obtain various summary statistics that exclude NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "rpzktpYPLIei",
    "outputId": "eb310bad-7b86-474b-8ab0-8736445c6917"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5572</td>\n",
       "      <td>5572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>5169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>ham</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4825</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0                       1\n",
       "count   5572                    5572\n",
       "unique     2                    5169\n",
       "top      ham  Sorry, I'll call later\n",
       "freq    4825                      30"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AyubgQxRK7aU"
   },
   "source": [
    "### Data Info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xoBb6vu9NLbz"
   },
   "source": [
    "Here, we have a collection of text data known as a corpus. Specifically, there are 5,572 SMS messages written in English, serving as training examples. The first column is the target variable containing the class labels, which tells us if the message is spam or ham (aka not spam). The second column is the SMS message itself, stored as a string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CM6wT7MWWLdG"
   },
   "source": [
    "Since the target variable contains discrete values, this is a **classification** task. Let's start by placing the target variable in its own table and checking out how the two classes are distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "AL77rJf7NfVW",
    "outputId": "4730bc59-3064-45e8-efa0-ab7dc254392d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     4825\n",
       "spam     747\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = sms[0]\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TPrTvN3mNNOq"
   },
   "source": [
    "It looks like there are far fewer training examples for spam than ham—we'll take this imbalance into account in the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m8CwsiEsWdD-"
   },
   "source": [
    "In addition, we need to encode the class labels in the target variable as numbers to ensure compatibility with some models in Scikit-learn. Because we have binary classes, let's use LabelEncoder and set 'spam' = 1 and 'ham' = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "08MzZah8XI4e"
   },
   "source": [
    "LabelEncoder is a function of scikit learn's preprocessing capabilities, which helps to encode target labels with values between 0 and the (# of classes) - 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PJxyWauaXInG"
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nWm_nsPzLFlN"
   },
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "y_enc = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "NtX_JCKBXjZ7",
    "outputId": "155d8c37-f8ef-4d30-abce-c8d2cd4c1792"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nxdAQXrjWkr1"
   },
   "source": [
    "Next, we place the SMS message data into its own table. We must convert this corpus into useful numerical features so we can train this classifier and this is where NLP works its magic!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1PYEcisJW8zG"
   },
   "outputs": [],
   "source": [
    "raw_text = sms[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "b6UsVe0OW69q",
    "outputId": "87bd4c0f-006b-45f5-99a8-5a6c62294dd6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Go until jurong point, crazy.. Available only ...\n",
       "1                           Ok lar... Joking wif u oni...\n",
       "2       Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3       U dun say so early hor... U c already then say...\n",
       "4       Nah I don't think he goes to usf, he lives aro...\n",
       "                              ...                        \n",
       "5567    This is the 2nd time we have tried 2 contact u...\n",
       "5568                 Will ü b going to esplanade fr home?\n",
       "5569    Pity, * was in mood for that. So...any other s...\n",
       "5570    The guy did some bitching but I acted like i'd...\n",
       "5571                           Rofl. Its true to its name\n",
       "Name: 1, Length: 5572, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CIdoXqLaPY7O"
   },
   "source": [
    "Another important part of any dataset is missing values. When this happens, the dataset can lose expressiveness, which may lead to weak or at times biased analyses. Practically, this means that when you’re missing values for certain features, the chances of your classification or predictions for the data being off only increase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iwXB_McsPjqC"
   },
   "source": [
    "To identify the rows that contain missing values, you can use isnull(). In the result that you’ll get back, you’ll see True or False appearing in each cell: True will indicate that the value contained within the cell is a missing value, False means that the cell contains a ‘normal’ value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "colab_type": "code",
    "id": "gvdbliy1PkCt",
    "outputId": "3c62d14d-6dca-48fc-f96f-c1f08e1be613"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0      1\n",
       "0     False  False\n",
       "1     False  False\n",
       "2     False  False\n",
       "3     False  False\n",
       "4     False  False\n",
       "...     ...    ...\n",
       "5567  False  False\n",
       "5568  False  False\n",
       "5569  False  False\n",
       "5570  False  False\n",
       "5571  False  False\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.isnull(sms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xHLGVRvBPuox"
   },
   "source": [
    "In this case, you see that the data is quite complete: there are no missing values and you're lucky! But this is definitely not always the case for datasets out there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HZgFXvPeK731"
   },
   "source": [
    "### Basic Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pLqnQ-KuQEu7"
   },
   "source": [
    "Data visualization can help with identifying patterns in the data. The Python libraries Seaborn and Matplotlib are easy and quick ways to do this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "Smz8QSJBLEJg",
    "outputId": "d6006510-f447-4984-8a4e-090e1545b539"
   },
   "outputs": [],
   "source": [
    "import matplotlib as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "52s9-Oc1XvgX"
   },
   "source": [
    "There are a couple basic visualizations we can do. The first is displaying the length of all the dataset instances. To do this, we must first label the columns with their appropriate titles and add a column to the dataset that contains the length. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pD-0ZhgOqPZ5"
   },
   "outputs": [],
   "source": [
    "sms.columns=['label', 'msg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "YeEVeKxZq2c3",
    "outputId": "f3c01e66-c649-4864-d5a4-1266dddcf560"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>msg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                                msg\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6f02B7Y6Y--F"
   },
   "outputs": [],
   "source": [
    "sms[\"length\"] = sms[\"msg\"].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "-ZvGSe3Kq9Q9",
    "outputId": "112e2db7-381c-4639-c71b-8d97ef126f2b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>msg</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                                msg  length\n",
       "0   ham  Go until jurong point, crazy.. Available only ...     111\n",
       "1   ham                      Ok lar... Joking wif u oni...      29\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     155\n",
       "3   ham  U dun say so early hor... U c already then say...      49\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...      61"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "colab_type": "code",
    "id": "9tGjoINoXvJM",
    "outputId": "c62deb2e-82f6-489c-a84b-9db899f1b3e0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chara\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\seaborn\\distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='length'>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(sms[\"length\"], kde=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PZEmaQyrLIzI"
   },
   "source": [
    "## Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cwnXlxfWrUoR"
   },
   "source": [
    "There are many feature engineering strategies for transforming text data into features. NLP problem requires a tailored approach to determine which terms are relevant and meaningful, and this is where we begin our pre-processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wXeOxA2vJowM"
   },
   "source": [
    "### Step 1: Contraction Mapping / Expanding Contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kK3mBeKq3Sn-"
   },
   "source": [
    "Contractions are words that we write with an apostrophe. Examples of contractions are words like “ain’t” or “aren’t”. Since we want to standardize our text, it makes sense to expand these contractions. We are going to add a new column to our dataframe called “no_contract” and apply a lambda function to the \"msg\" field which will expand any contractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "_bIyOL3A3sZz",
    "outputId": "8a31158b-fef2-4467-e6ba-ffd755d27e5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting contractions\n",
      "  Downloading contractions-0.1.68-py2.py3-none-any.whl (8.1 kB)\n",
      "Collecting textsearch>=0.0.21\n",
      "  Downloading textsearch-0.0.21-py2.py3-none-any.whl (7.5 kB)\n",
      "Collecting anyascii\n",
      "  Downloading anyascii-0.3.0-py3-none-any.whl (284 kB)\n",
      "Collecting pyahocorasick\n",
      "  Downloading pyahocorasick-1.4.4-cp39-cp39-win_amd64.whl (39 kB)\n",
      "Installing collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
      "Successfully installed anyascii-0.3.0 contractions-0.1.68 pyahocorasick-1.4.4 textsearch-0.0.21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\chara\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install contractions\n",
    "import contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WEKvXDOH31l7"
   },
   "outputs": [],
   "source": [
    "sms['no_contract'] = sms['msg'].apply(lambda x: [contractions.fix(word) for word in x.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "cRud_Vrk7Ggf",
    "outputId": "23cbe431-31a5-4a01-82bf-45c22363ff44"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>msg</th>\n",
       "      <th>length</th>\n",
       "      <th>no_contract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "      <td>[Go, until, jurong, point,, crazy.., Available...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>29</td>\n",
       "      <td>[Ok, lar..., Joking, wif, you, oni...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>155</td>\n",
       "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>49</td>\n",
       "      <td>[YOU, dun, say, so, early, hor..., YOU, c, alr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>61</td>\n",
       "      <td>[Nah, I, do not, think, he, goes, to, usf,, he...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                                msg  length  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...     111   \n",
       "1   ham                      Ok lar... Joking wif u oni...      29   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     155   \n",
       "3   ham  U dun say so early hor... U c already then say...      49   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...      61   \n",
       "\n",
       "                                         no_contract  \n",
       "0  [Go, until, jurong, point,, crazy.., Available...  \n",
       "1             [Ok, lar..., Joking, wif, you, oni...]  \n",
       "2  [Free, entry, in, 2, a, wkly, comp, to, win, F...  \n",
       "3  [YOU, dun, say, so, early, hor..., YOU, c, alr...  \n",
       "4  [Nah, I, do not, think, he, goes, to, usf,, he...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VzTj7U_J7USX"
   },
   "outputs": [],
   "source": [
    "sms[\"msg_str\"] = [' '.join(map(str, l)) for l in sms['no_contract']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "sO4OPskH7gRf",
    "outputId": "ec5a0623-5889-4a2a-c38d-83c8de493afa"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>msg</th>\n",
       "      <th>length</th>\n",
       "      <th>no_contract</th>\n",
       "      <th>msg_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "      <td>[Go, until, jurong, point,, crazy.., Available...</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>29</td>\n",
       "      <td>[Ok, lar..., Joking, wif, you, oni...]</td>\n",
       "      <td>Ok lar... Joking wif you oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>155</td>\n",
       "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>49</td>\n",
       "      <td>[YOU, dun, say, so, early, hor..., YOU, c, alr...</td>\n",
       "      <td>YOU dun say so early hor... YOU c already then...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>61</td>\n",
       "      <td>[Nah, I, do not, think, he, goes, to, usf,, he...</td>\n",
       "      <td>Nah I do not think he goes to usf, he lives ar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                                msg  length  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...     111   \n",
       "1   ham                      Ok lar... Joking wif u oni...      29   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     155   \n",
       "3   ham  U dun say so early hor... U c already then say...      49   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...      61   \n",
       "\n",
       "                                         no_contract  \\\n",
       "0  [Go, until, jurong, point,, crazy.., Available...   \n",
       "1             [Ok, lar..., Joking, wif, you, oni...]   \n",
       "2  [Free, entry, in, 2, a, wkly, comp, to, win, F...   \n",
       "3  [YOU, dun, say, so, early, hor..., YOU, c, alr...   \n",
       "4  [Nah, I, do not, think, he, goes, to, usf,, he...   \n",
       "\n",
       "                                             msg_str  \n",
       "0  Go until jurong point, crazy.. Available only ...  \n",
       "1                    Ok lar... Joking wif you oni...  \n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...  \n",
       "3  YOU dun say so early hor... YOU c already then...  \n",
       "4  Nah I do not think he goes to usf, he lives ar...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6fD7Vjr8JlEI"
   },
   "source": [
    "### Step 2: Tokenization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XF5N0vquuMV1"
   },
   "source": [
    "In this step, we construct the features. We will begin by breaking apart the corpus into a vocabulary of unique terms, and this is called tokanization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z_W-ztrPuUcm"
   },
   "source": [
    "We can tokenize individual terms and generate what's called a bag of words model. We can also tokenize using nltk, which is the leading platform for building Python programs to work with human language data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "PCm7QGpCTnld",
    "outputId": "d11edd13-e750-407b-a941-f67c37c11392"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\chara\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oP2etJNC-a-E"
   },
   "source": [
    "Now, we can apply the tokenizer to our dataset. We will apply NLTK.word_tokenize() function to the “msg_str” column and create a new column named “tokenized”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "aXKZEWZG-gII",
    "outputId": "b36cac0b-6965-4b7f-93ee-692189b2a44c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>msg</th>\n",
       "      <th>length</th>\n",
       "      <th>no_contract</th>\n",
       "      <th>msg_str</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "      <td>[Go, until, jurong, point,, crazy.., Available...</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>[Go, until, jurong, point, ,, crazy, .., Avail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>29</td>\n",
       "      <td>[Ok, lar..., Joking, wif, you, oni...]</td>\n",
       "      <td>Ok lar... Joking wif you oni...</td>\n",
       "      <td>[Ok, lar, ..., Joking, wif, you, oni, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>155</td>\n",
       "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>49</td>\n",
       "      <td>[YOU, dun, say, so, early, hor..., YOU, c, alr...</td>\n",
       "      <td>YOU dun say so early hor... YOU c already then...</td>\n",
       "      <td>[YOU, dun, say, so, early, hor, ..., YOU, c, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>61</td>\n",
       "      <td>[Nah, I, do not, think, he, goes, to, usf,, he...</td>\n",
       "      <td>Nah I do not think he goes to usf, he lives ar...</td>\n",
       "      <td>[Nah, I, do, not, think, he, goes, to, usf, ,,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                                msg  length  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...     111   \n",
       "1   ham                      Ok lar... Joking wif u oni...      29   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     155   \n",
       "3   ham  U dun say so early hor... U c already then say...      49   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...      61   \n",
       "\n",
       "                                         no_contract  \\\n",
       "0  [Go, until, jurong, point,, crazy.., Available...   \n",
       "1             [Ok, lar..., Joking, wif, you, oni...]   \n",
       "2  [Free, entry, in, 2, a, wkly, comp, to, win, F...   \n",
       "3  [YOU, dun, say, so, early, hor..., YOU, c, alr...   \n",
       "4  [Nah, I, do not, think, he, goes, to, usf,, he...   \n",
       "\n",
       "                                             msg_str  \\\n",
       "0  Go until jurong point, crazy.. Available only ...   \n",
       "1                    Ok lar... Joking wif you oni...   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3  YOU dun say so early hor... YOU c already then...   \n",
       "4  Nah I do not think he goes to usf, he lives ar...   \n",
       "\n",
       "                                           tokenized  \n",
       "0  [Go, until, jurong, point, ,, crazy, .., Avail...  \n",
       "1         [Ok, lar, ..., Joking, wif, you, oni, ...]  \n",
       "2  [Free, entry, in, 2, a, wkly, comp, to, win, F...  \n",
       "3  [YOU, dun, say, so, early, hor, ..., YOU, c, a...  \n",
       "4  [Nah, I, do, not, think, he, goes, to, usf, ,,...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms['tokenized'] = sms['msg_str'].apply(word_tokenize)\n",
    "sms.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_dSlUTVNJjAU"
   },
   "source": [
    "### Step 3: Noise Cleaning - spacing, special characters, lowercasing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6eTvf_eKrrEw"
   },
   "source": [
    "Let'a take a small step back and examine a few random examples of SMS messages from our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 589
    },
    "colab_type": "code",
    "id": "QZAoKyvnrxmB",
    "outputId": "88232196-22cb-4a42-e433-9bd33ecdfe72"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>msg</th>\n",
       "      <th>length</th>\n",
       "      <th>no_contract</th>\n",
       "      <th>msg_str</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2971</th>\n",
       "      <td>ham</td>\n",
       "      <td>U should make a fb list</td>\n",
       "      <td>23</td>\n",
       "      <td>[YOU, should, make, a, fb, list]</td>\n",
       "      <td>YOU should make a fb list</td>\n",
       "      <td>[YOU, should, make, a, fb, list]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3092</th>\n",
       "      <td>spam</td>\n",
       "      <td>LORD OF THE RINGS:RETURN OF THE KING in store ...</td>\n",
       "      <td>140</td>\n",
       "      <td>[LORD, OF, THE, RINGS:RETURN, OF, THE, KING, i...</td>\n",
       "      <td>LORD OF THE RINGS:RETURN OF THE KING in store ...</td>\n",
       "      <td>[LORD, OF, THE, RINGS, :, RETURN, OF, THE, KIN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1927</th>\n",
       "      <td>ham</td>\n",
       "      <td>Dont give a monkeys wot they think and i certa...</td>\n",
       "      <td>135</td>\n",
       "      <td>[Do Not, give, a, monkeys, wot, they, think, a...</td>\n",
       "      <td>Do Not give a monkeys wot they think and i cer...</td>\n",
       "      <td>[Do, Not, give, a, monkeys, wot, they, think, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3714</th>\n",
       "      <td>ham</td>\n",
       "      <td>If i not meeting ü all rite then i'll go home ...</td>\n",
       "      <td>84</td>\n",
       "      <td>[If, i, not, meeting, ü, all, rite, then, i wi...</td>\n",
       "      <td>If i not meeting ü all rite then i will go hom...</td>\n",
       "      <td>[If, i, not, meeting, ü, all, rite, then, i, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5476</th>\n",
       "      <td>ham</td>\n",
       "      <td>Yes princess! I want to please you every night...</td>\n",
       "      <td>74</td>\n",
       "      <td>[Yes, princess!, I, want, to, please, you, eve...</td>\n",
       "      <td>Yes princess! I want to please you every night...</td>\n",
       "      <td>[Yes, princess, !, I, want, to, please, you, e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>ham</td>\n",
       "      <td>HEY GIRL. HOW R U? HOPE U R WELL ME AN DEL R B...</td>\n",
       "      <td>107</td>\n",
       "      <td>[HEY, GIRL., HOW, R, YOU?, HOPE, YOU, R, WELL,...</td>\n",
       "      <td>HEY GIRL. HOW R YOU? HOPE YOU R WELL ME AN DEL...</td>\n",
       "      <td>[HEY, GIRL, ., HOW, R, YOU, ?, HOPE, YOU, R, W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>ham</td>\n",
       "      <td>LOL ... Have you made plans for new years?</td>\n",
       "      <td>42</td>\n",
       "      <td>[LOL, ..., Have, you, made, plans, for, new, y...</td>\n",
       "      <td>LOL ... Have you made plans for new years?</td>\n",
       "      <td>[LOL, ..., Have, you, made, plans, for, new, y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5193</th>\n",
       "      <td>ham</td>\n",
       "      <td>It's wylie, you in tampa or sarasota?</td>\n",
       "      <td>37</td>\n",
       "      <td>[It is, wylie,, you, in, tampa, or, sarasota?]</td>\n",
       "      <td>It is wylie, you in tampa or sarasota?</td>\n",
       "      <td>[It, is, wylie, ,, you, in, tampa, or, sarasot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1799</th>\n",
       "      <td>ham</td>\n",
       "      <td>That one week leave i put know that time. Why.</td>\n",
       "      <td>46</td>\n",
       "      <td>[That, one, week, leave, i, put, know, that, t...</td>\n",
       "      <td>That one week leave i put know that time. Why.</td>\n",
       "      <td>[That, one, week, leave, i, put, know, that, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1933</th>\n",
       "      <td>ham</td>\n",
       "      <td>Jus finished avatar nigro</td>\n",
       "      <td>25</td>\n",
       "      <td>[Jus, finished, avatar, nigro]</td>\n",
       "      <td>Jus finished avatar nigro</td>\n",
       "      <td>[Jus, finished, avatar, nigro]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>279 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                                msg  length  \\\n",
       "2971   ham                            U should make a fb list      23   \n",
       "3092  spam  LORD OF THE RINGS:RETURN OF THE KING in store ...     140   \n",
       "1927   ham  Dont give a monkeys wot they think and i certa...     135   \n",
       "3714   ham  If i not meeting ü all rite then i'll go home ...      84   \n",
       "5476   ham  Yes princess! I want to please you every night...      74   \n",
       "...    ...                                                ...     ...   \n",
       "129    ham  HEY GIRL. HOW R U? HOPE U R WELL ME AN DEL R B...     107   \n",
       "449    ham         LOL ... Have you made plans for new years?      42   \n",
       "5193   ham              It's wylie, you in tampa or sarasota?      37   \n",
       "1799   ham     That one week leave i put know that time. Why.      46   \n",
       "1933   ham                          Jus finished avatar nigro      25   \n",
       "\n",
       "                                            no_contract  \\\n",
       "2971                   [YOU, should, make, a, fb, list]   \n",
       "3092  [LORD, OF, THE, RINGS:RETURN, OF, THE, KING, i...   \n",
       "1927  [Do Not, give, a, monkeys, wot, they, think, a...   \n",
       "3714  [If, i, not, meeting, ü, all, rite, then, i wi...   \n",
       "5476  [Yes, princess!, I, want, to, please, you, eve...   \n",
       "...                                                 ...   \n",
       "129   [HEY, GIRL., HOW, R, YOU?, HOPE, YOU, R, WELL,...   \n",
       "449   [LOL, ..., Have, you, made, plans, for, new, y...   \n",
       "5193     [It is, wylie,, you, in, tampa, or, sarasota?]   \n",
       "1799  [That, one, week, leave, i, put, know, that, t...   \n",
       "1933                     [Jus, finished, avatar, nigro]   \n",
       "\n",
       "                                                msg_str  \\\n",
       "2971                          YOU should make a fb list   \n",
       "3092  LORD OF THE RINGS:RETURN OF THE KING in store ...   \n",
       "1927  Do Not give a monkeys wot they think and i cer...   \n",
       "3714  If i not meeting ü all rite then i will go hom...   \n",
       "5476  Yes princess! I want to please you every night...   \n",
       "...                                                 ...   \n",
       "129   HEY GIRL. HOW R YOU? HOPE YOU R WELL ME AN DEL...   \n",
       "449          LOL ... Have you made plans for new years?   \n",
       "5193             It is wylie, you in tampa or sarasota?   \n",
       "1799     That one week leave i put know that time. Why.   \n",
       "1933                          Jus finished avatar nigro   \n",
       "\n",
       "                                              tokenized  \n",
       "2971                   [YOU, should, make, a, fb, list]  \n",
       "3092  [LORD, OF, THE, RINGS, :, RETURN, OF, THE, KIN...  \n",
       "1927  [Do, Not, give, a, monkeys, wot, they, think, ...  \n",
       "3714  [If, i, not, meeting, ü, all, rite, then, i, w...  \n",
       "5476  [Yes, princess, !, I, want, to, please, you, e...  \n",
       "...                                                 ...  \n",
       "129   [HEY, GIRL, ., HOW, R, YOU, ?, HOPE, YOU, R, W...  \n",
       "449   [LOL, ..., Have, you, made, plans, for, new, y...  \n",
       "5193  [It, is, wylie, ,, you, in, tampa, or, sarasot...  \n",
       "1799  [That, one, week, leave, i, put, know, that, t...  \n",
       "1933                     [Jus, finished, avatar, nigro]  \n",
       "\n",
       "[279 rows x 6 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms.sample(frac=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C-9loSCBsnKL"
   },
   "source": [
    "Transforming all words to lowercase is also a very common pre-processing step. In this case, we will once again append a new column named “lower” to the dataframe which will transform all the tokenized words into lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "id": "EGeQt7LCsuQi",
    "outputId": "a762c48e-64f7-4824-da8c-88ec3e499154"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>msg</th>\n",
       "      <th>length</th>\n",
       "      <th>no_contract</th>\n",
       "      <th>msg_str</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>lower</th>\n",
       "      <th>no_punc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "      <td>[Go, until, jurong, point,, crazy.., Available...</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>[Go, until, jurong, point, ,, crazy, .., Avail...</td>\n",
       "      <td>[go, until, jurong, point, ,, crazy, .., avail...</td>\n",
       "      <td>[go, until, jurong, point, crazy, .., availabl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>29</td>\n",
       "      <td>[Ok, lar..., Joking, wif, you, oni...]</td>\n",
       "      <td>Ok lar... Joking wif you oni...</td>\n",
       "      <td>[Ok, lar, ..., Joking, wif, you, oni, ...]</td>\n",
       "      <td>[ok, lar, ..., joking, wif, you, oni, ...]</td>\n",
       "      <td>[ok, lar, ..., joking, wif, you, oni, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>155</td>\n",
       "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>49</td>\n",
       "      <td>[YOU, dun, say, so, early, hor..., YOU, c, alr...</td>\n",
       "      <td>YOU dun say so early hor... YOU c already then...</td>\n",
       "      <td>[YOU, dun, say, so, early, hor, ..., YOU, c, a...</td>\n",
       "      <td>[you, dun, say, so, early, hor, ..., you, c, a...</td>\n",
       "      <td>[you, dun, say, so, early, hor, ..., you, c, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>61</td>\n",
       "      <td>[Nah, I, do not, think, he, goes, to, usf,, he...</td>\n",
       "      <td>Nah I do not think he goes to usf, he lives ar...</td>\n",
       "      <td>[Nah, I, do, not, think, he, goes, to, usf, ,,...</td>\n",
       "      <td>[nah, i, do, not, think, he, goes, to, usf, ,,...</td>\n",
       "      <td>[nah, i, do, not, think, he, goes, to, usf, he...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                                msg  length  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...     111   \n",
       "1   ham                      Ok lar... Joking wif u oni...      29   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     155   \n",
       "3   ham  U dun say so early hor... U c already then say...      49   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...      61   \n",
       "\n",
       "                                         no_contract  \\\n",
       "0  [Go, until, jurong, point,, crazy.., Available...   \n",
       "1             [Ok, lar..., Joking, wif, you, oni...]   \n",
       "2  [Free, entry, in, 2, a, wkly, comp, to, win, F...   \n",
       "3  [YOU, dun, say, so, early, hor..., YOU, c, alr...   \n",
       "4  [Nah, I, do not, think, he, goes, to, usf,, he...   \n",
       "\n",
       "                                             msg_str  \\\n",
       "0  Go until jurong point, crazy.. Available only ...   \n",
       "1                    Ok lar... Joking wif you oni...   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3  YOU dun say so early hor... YOU c already then...   \n",
       "4  Nah I do not think he goes to usf, he lives ar...   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  [Go, until, jurong, point, ,, crazy, .., Avail...   \n",
       "1         [Ok, lar, ..., Joking, wif, you, oni, ...]   \n",
       "2  [Free, entry, in, 2, a, wkly, comp, to, win, F...   \n",
       "3  [YOU, dun, say, so, early, hor, ..., YOU, c, a...   \n",
       "4  [Nah, I, do, not, think, he, goes, to, usf, ,,...   \n",
       "\n",
       "                                               lower  \\\n",
       "0  [go, until, jurong, point, ,, crazy, .., avail...   \n",
       "1         [ok, lar, ..., joking, wif, you, oni, ...]   \n",
       "2  [free, entry, in, 2, a, wkly, comp, to, win, f...   \n",
       "3  [you, dun, say, so, early, hor, ..., you, c, a...   \n",
       "4  [nah, i, do, not, think, he, goes, to, usf, ,,...   \n",
       "\n",
       "                                             no_punc  \n",
       "0  [go, until, jurong, point, crazy, .., availabl...  \n",
       "1         [ok, lar, ..., joking, wif, you, oni, ...]  \n",
       "2  [free, entry, in, 2, a, wkly, comp, to, win, f...  \n",
       "3  [you, dun, say, so, early, hor, ..., you, c, a...  \n",
       "4  [nah, i, do, not, think, he, goes, to, usf, he...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms['lower'] = sms['tokenized'].apply(lambda x: [word.lower() for word in x])\n",
    "sms.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FAovmw7AsxCt"
   },
   "source": [
    "Next, we'll remove all punctuation since they serve little value once we begin to analyze our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "fmK_8wVts0ql",
    "outputId": "69865a7e-50be-4314-e94c-6d0fa51c3d31"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>msg</th>\n",
       "      <th>length</th>\n",
       "      <th>no_contract</th>\n",
       "      <th>msg_str</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>lower</th>\n",
       "      <th>no_punc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "      <td>[Go, until, jurong, point,, crazy.., Available...</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>[Go, until, jurong, point, ,, crazy, .., Avail...</td>\n",
       "      <td>[go, until, jurong, point, ,, crazy, .., avail...</td>\n",
       "      <td>[go, until, jurong, point, crazy, .., availabl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>29</td>\n",
       "      <td>[Ok, lar..., Joking, wif, you, oni...]</td>\n",
       "      <td>Ok lar... Joking wif you oni...</td>\n",
       "      <td>[Ok, lar, ..., Joking, wif, you, oni, ...]</td>\n",
       "      <td>[ok, lar, ..., joking, wif, you, oni, ...]</td>\n",
       "      <td>[ok, lar, ..., joking, wif, you, oni, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>155</td>\n",
       "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>49</td>\n",
       "      <td>[YOU, dun, say, so, early, hor..., YOU, c, alr...</td>\n",
       "      <td>YOU dun say so early hor... YOU c already then...</td>\n",
       "      <td>[YOU, dun, say, so, early, hor, ..., YOU, c, a...</td>\n",
       "      <td>[you, dun, say, so, early, hor, ..., you, c, a...</td>\n",
       "      <td>[you, dun, say, so, early, hor, ..., you, c, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>61</td>\n",
       "      <td>[Nah, I, do not, think, he, goes, to, usf,, he...</td>\n",
       "      <td>Nah I do not think he goes to usf, he lives ar...</td>\n",
       "      <td>[Nah, I, do, not, think, he, goes, to, usf, ,,...</td>\n",
       "      <td>[nah, i, do, not, think, he, goes, to, usf, ,,...</td>\n",
       "      <td>[nah, i, do, not, think, he, goes, to, usf, he...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                                msg  length  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...     111   \n",
       "1   ham                      Ok lar... Joking wif u oni...      29   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     155   \n",
       "3   ham  U dun say so early hor... U c already then say...      49   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...      61   \n",
       "\n",
       "                                         no_contract  \\\n",
       "0  [Go, until, jurong, point,, crazy.., Available...   \n",
       "1             [Ok, lar..., Joking, wif, you, oni...]   \n",
       "2  [Free, entry, in, 2, a, wkly, comp, to, win, F...   \n",
       "3  [YOU, dun, say, so, early, hor..., YOU, c, alr...   \n",
       "4  [Nah, I, do not, think, he, goes, to, usf,, he...   \n",
       "\n",
       "                                             msg_str  \\\n",
       "0  Go until jurong point, crazy.. Available only ...   \n",
       "1                    Ok lar... Joking wif you oni...   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3  YOU dun say so early hor... YOU c already then...   \n",
       "4  Nah I do not think he goes to usf, he lives ar...   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  [Go, until, jurong, point, ,, crazy, .., Avail...   \n",
       "1         [Ok, lar, ..., Joking, wif, you, oni, ...]   \n",
       "2  [Free, entry, in, 2, a, wkly, comp, to, win, F...   \n",
       "3  [YOU, dun, say, so, early, hor, ..., YOU, c, a...   \n",
       "4  [Nah, I, do, not, think, he, goes, to, usf, ,,...   \n",
       "\n",
       "                                               lower  \\\n",
       "0  [go, until, jurong, point, ,, crazy, .., avail...   \n",
       "1         [ok, lar, ..., joking, wif, you, oni, ...]   \n",
       "2  [free, entry, in, 2, a, wkly, comp, to, win, f...   \n",
       "3  [you, dun, say, so, early, hor, ..., you, c, a...   \n",
       "4  [nah, i, do, not, think, he, goes, to, usf, ,,...   \n",
       "\n",
       "                                             no_punc  \n",
       "0  [go, until, jurong, point, crazy, .., availabl...  \n",
       "1         [ok, lar, ..., joking, wif, you, oni, ...]  \n",
       "2  [free, entry, in, 2, a, wkly, comp, to, win, f...  \n",
       "3  [you, dun, say, so, early, hor, ..., you, c, a...  \n",
       "4  [nah, i, do, not, think, he, goes, to, usf, he...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "punc = string.punctuation\n",
    "sms['no_punc'] = sms['lower'].apply(lambda x: [word for word in x if word not in punc])\n",
    "sms.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OKiy3apFJqeb"
   },
   "source": [
    "### Step 4: ‘Stop Words’ Identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g-rMODaGtBlV"
   },
   "source": [
    "Some words in the English language, while necessary, don't contribute much to the meaning of a phrase. These words, such as \"when\", \"had\", \"those\" or \"before\", are called stop words and should be filtered out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qGzWZxk-8Pil"
   },
   "source": [
    "First, we need to import the NLTK stopwords library and set our stopwords to “english”. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "_IlsjVG-LLWX",
    "outputId": "2c1b8f23-49f7-4d67-d19a-fd0e33a7ddf5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\chara\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n73ZUUKrtOcP"
   },
   "source": [
    "We are going to add a new column “no_stopwords” which will remove the stopwords from the “no_punc” column since it has been tokenized, had been converted to lowercase and punctuation was removed. Once again a for-loop within a lambda function will iterate over the tokens in “no_punc” and only return the tokens which do not exist in our “stop_words” variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "colab_type": "code",
    "id": "NMwO1hBltQXZ",
    "outputId": "d00ab477-a79a-49ff-9852-fc817a724cba"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>msg</th>\n",
       "      <th>length</th>\n",
       "      <th>no_contract</th>\n",
       "      <th>msg_str</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>lower</th>\n",
       "      <th>no_punc</th>\n",
       "      <th>stopwords_removed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "      <td>[Go, until, jurong, point,, crazy.., Available...</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>[Go, until, jurong, point, ,, crazy, .., Avail...</td>\n",
       "      <td>[go, until, jurong, point, ,, crazy, .., avail...</td>\n",
       "      <td>[go, until, jurong, point, crazy, .., availabl...</td>\n",
       "      <td>[go, jurong, point, crazy, .., available, bugi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>29</td>\n",
       "      <td>[Ok, lar..., Joking, wif, you, oni...]</td>\n",
       "      <td>Ok lar... Joking wif you oni...</td>\n",
       "      <td>[Ok, lar, ..., Joking, wif, you, oni, ...]</td>\n",
       "      <td>[ok, lar, ..., joking, wif, you, oni, ...]</td>\n",
       "      <td>[ok, lar, ..., joking, wif, you, oni, ...]</td>\n",
       "      <td>[ok, lar, ..., joking, wif, oni, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>155</td>\n",
       "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>49</td>\n",
       "      <td>[YOU, dun, say, so, early, hor..., YOU, c, alr...</td>\n",
       "      <td>YOU dun say so early hor... YOU c already then...</td>\n",
       "      <td>[YOU, dun, say, so, early, hor, ..., YOU, c, a...</td>\n",
       "      <td>[you, dun, say, so, early, hor, ..., you, c, a...</td>\n",
       "      <td>[you, dun, say, so, early, hor, ..., you, c, a...</td>\n",
       "      <td>[dun, say, early, hor, ..., c, already, say, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>61</td>\n",
       "      <td>[Nah, I, do not, think, he, goes, to, usf,, he...</td>\n",
       "      <td>Nah I do not think he goes to usf, he lives ar...</td>\n",
       "      <td>[Nah, I, do, not, think, he, goes, to, usf, ,,...</td>\n",
       "      <td>[nah, i, do, not, think, he, goes, to, usf, ,,...</td>\n",
       "      <td>[nah, i, do, not, think, he, goes, to, usf, he...</td>\n",
       "      <td>[nah, think, goes, usf, lives, around, though]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                                msg  length  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...     111   \n",
       "1   ham                      Ok lar... Joking wif u oni...      29   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     155   \n",
       "3   ham  U dun say so early hor... U c already then say...      49   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...      61   \n",
       "\n",
       "                                         no_contract  \\\n",
       "0  [Go, until, jurong, point,, crazy.., Available...   \n",
       "1             [Ok, lar..., Joking, wif, you, oni...]   \n",
       "2  [Free, entry, in, 2, a, wkly, comp, to, win, F...   \n",
       "3  [YOU, dun, say, so, early, hor..., YOU, c, alr...   \n",
       "4  [Nah, I, do not, think, he, goes, to, usf,, he...   \n",
       "\n",
       "                                             msg_str  \\\n",
       "0  Go until jurong point, crazy.. Available only ...   \n",
       "1                    Ok lar... Joking wif you oni...   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3  YOU dun say so early hor... YOU c already then...   \n",
       "4  Nah I do not think he goes to usf, he lives ar...   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  [Go, until, jurong, point, ,, crazy, .., Avail...   \n",
       "1         [Ok, lar, ..., Joking, wif, you, oni, ...]   \n",
       "2  [Free, entry, in, 2, a, wkly, comp, to, win, F...   \n",
       "3  [YOU, dun, say, so, early, hor, ..., YOU, c, a...   \n",
       "4  [Nah, I, do, not, think, he, goes, to, usf, ,,...   \n",
       "\n",
       "                                               lower  \\\n",
       "0  [go, until, jurong, point, ,, crazy, .., avail...   \n",
       "1         [ok, lar, ..., joking, wif, you, oni, ...]   \n",
       "2  [free, entry, in, 2, a, wkly, comp, to, win, f...   \n",
       "3  [you, dun, say, so, early, hor, ..., you, c, a...   \n",
       "4  [nah, i, do, not, think, he, goes, to, usf, ,,...   \n",
       "\n",
       "                                             no_punc  \\\n",
       "0  [go, until, jurong, point, crazy, .., availabl...   \n",
       "1         [ok, lar, ..., joking, wif, you, oni, ...]   \n",
       "2  [free, entry, in, 2, a, wkly, comp, to, win, f...   \n",
       "3  [you, dun, say, so, early, hor, ..., you, c, a...   \n",
       "4  [nah, i, do, not, think, he, goes, to, usf, he...   \n",
       "\n",
       "                                   stopwords_removed  \n",
       "0  [go, jurong, point, crazy, .., available, bugi...  \n",
       "1              [ok, lar, ..., joking, wif, oni, ...]  \n",
       "2  [free, entry, 2, wkly, comp, win, fa, cup, fin...  \n",
       "3  [dun, say, early, hor, ..., c, already, say, ...]  \n",
       "4     [nah, think, goes, usf, lives, around, though]  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms['stopwords_removed'] = sms['no_punc'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "sms.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M7ZrTjxJJqqn"
   },
   "source": [
    "### Step 5: Stemming/Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XPMcw9E7tpqe"
   },
   "source": [
    "The idea of stemming is to reduce different forms of word usage into its root word. For example, “drive”, “drove”, “driving”, “driven”, “driver” are derivatives of the word “drive” and very often researchers want to remove this variability from their corpus. Compared to lemmatization, stemming is certainly the less complicated method but it often does not produce a dictionary-specific morphological root of the word. In other words, stemming the word “pies” will often produce a root of “pi” whereas lemmatization will find the morphological root of “pie”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A0jD4XRn_AYM"
   },
   "source": [
    "Instead of taking the easy way out with stemming, let’s apply lemmatization to our data but it requires some additional steps compared to stemming. First, we have to apply parts of speech tags, in other words, determine the part of speech (ie. noun, verb, adverb, etc.) for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "_gOKB7uD_Zz8",
    "outputId": "f8f4bb09-7971-42db-eaba-052123e0e839"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\chara\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 629
    },
    "colab_type": "code",
    "id": "aX-fBFBdtqCI",
    "outputId": "cb407895-80ba-4954-8c78-559d8fab68fc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>msg</th>\n",
       "      <th>length</th>\n",
       "      <th>no_contract</th>\n",
       "      <th>msg_str</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>lower</th>\n",
       "      <th>no_punc</th>\n",
       "      <th>stopwords_removed</th>\n",
       "      <th>pos_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "      <td>[Go, until, jurong, point,, crazy.., Available...</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>[Go, until, jurong, point, ,, crazy, .., Avail...</td>\n",
       "      <td>[go, until, jurong, point, ,, crazy, .., avail...</td>\n",
       "      <td>[go, until, jurong, point, crazy, .., availabl...</td>\n",
       "      <td>[go, jurong, point, crazy, .., available, bugi...</td>\n",
       "      <td>[(go, VB), (jurong, JJ), (point, NN), (crazy, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>29</td>\n",
       "      <td>[Ok, lar..., Joking, wif, you, oni...]</td>\n",
       "      <td>Ok lar... Joking wif you oni...</td>\n",
       "      <td>[Ok, lar, ..., Joking, wif, you, oni, ...]</td>\n",
       "      <td>[ok, lar, ..., joking, wif, you, oni, ...]</td>\n",
       "      <td>[ok, lar, ..., joking, wif, you, oni, ...]</td>\n",
       "      <td>[ok, lar, ..., joking, wif, oni, ...]</td>\n",
       "      <td>[(ok, JJ), (lar, NN), (..., :), (joking, VBG),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>155</td>\n",
       "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, fin...</td>\n",
       "      <td>[(free, JJ), (entry, NN), (2, CD), (wkly, JJ),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>49</td>\n",
       "      <td>[YOU, dun, say, so, early, hor..., YOU, c, alr...</td>\n",
       "      <td>YOU dun say so early hor... YOU c already then...</td>\n",
       "      <td>[YOU, dun, say, so, early, hor, ..., YOU, c, a...</td>\n",
       "      <td>[you, dun, say, so, early, hor, ..., you, c, a...</td>\n",
       "      <td>[you, dun, say, so, early, hor, ..., you, c, a...</td>\n",
       "      <td>[dun, say, early, hor, ..., c, already, say, ...]</td>\n",
       "      <td>[(dun, NNS), (say, VBP), (early, JJ), (hor, NN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>61</td>\n",
       "      <td>[Nah, I, do not, think, he, goes, to, usf,, he...</td>\n",
       "      <td>Nah I do not think he goes to usf, he lives ar...</td>\n",
       "      <td>[Nah, I, do, not, think, he, goes, to, usf, ,,...</td>\n",
       "      <td>[nah, i, do, not, think, he, goes, to, usf, ,,...</td>\n",
       "      <td>[nah, i, do, not, think, he, goes, to, usf, he...</td>\n",
       "      <td>[nah, think, goes, usf, lives, around, though]</td>\n",
       "      <td>[(nah, RB), (think, NN), (goes, VBZ), (usf, JJ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                                msg  length  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...     111   \n",
       "1   ham                      Ok lar... Joking wif u oni...      29   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     155   \n",
       "3   ham  U dun say so early hor... U c already then say...      49   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...      61   \n",
       "\n",
       "                                         no_contract  \\\n",
       "0  [Go, until, jurong, point,, crazy.., Available...   \n",
       "1             [Ok, lar..., Joking, wif, you, oni...]   \n",
       "2  [Free, entry, in, 2, a, wkly, comp, to, win, F...   \n",
       "3  [YOU, dun, say, so, early, hor..., YOU, c, alr...   \n",
       "4  [Nah, I, do not, think, he, goes, to, usf,, he...   \n",
       "\n",
       "                                             msg_str  \\\n",
       "0  Go until jurong point, crazy.. Available only ...   \n",
       "1                    Ok lar... Joking wif you oni...   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3  YOU dun say so early hor... YOU c already then...   \n",
       "4  Nah I do not think he goes to usf, he lives ar...   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  [Go, until, jurong, point, ,, crazy, .., Avail...   \n",
       "1         [Ok, lar, ..., Joking, wif, you, oni, ...]   \n",
       "2  [Free, entry, in, 2, a, wkly, comp, to, win, F...   \n",
       "3  [YOU, dun, say, so, early, hor, ..., YOU, c, a...   \n",
       "4  [Nah, I, do, not, think, he, goes, to, usf, ,,...   \n",
       "\n",
       "                                               lower  \\\n",
       "0  [go, until, jurong, point, ,, crazy, .., avail...   \n",
       "1         [ok, lar, ..., joking, wif, you, oni, ...]   \n",
       "2  [free, entry, in, 2, a, wkly, comp, to, win, f...   \n",
       "3  [you, dun, say, so, early, hor, ..., you, c, a...   \n",
       "4  [nah, i, do, not, think, he, goes, to, usf, ,,...   \n",
       "\n",
       "                                             no_punc  \\\n",
       "0  [go, until, jurong, point, crazy, .., availabl...   \n",
       "1         [ok, lar, ..., joking, wif, you, oni, ...]   \n",
       "2  [free, entry, in, 2, a, wkly, comp, to, win, f...   \n",
       "3  [you, dun, say, so, early, hor, ..., you, c, a...   \n",
       "4  [nah, i, do, not, think, he, goes, to, usf, he...   \n",
       "\n",
       "                                   stopwords_removed  \\\n",
       "0  [go, jurong, point, crazy, .., available, bugi...   \n",
       "1              [ok, lar, ..., joking, wif, oni, ...]   \n",
       "2  [free, entry, 2, wkly, comp, win, fa, cup, fin...   \n",
       "3  [dun, say, early, hor, ..., c, already, say, ...]   \n",
       "4     [nah, think, goes, usf, lives, around, though]   \n",
       "\n",
       "                                            pos_tags  \n",
       "0  [(go, VB), (jurong, JJ), (point, NN), (crazy, ...  \n",
       "1  [(ok, JJ), (lar, NN), (..., :), (joking, VBG),...  \n",
       "2  [(free, JJ), (entry, NN), (2, CD), (wkly, JJ),...  \n",
       "3  [(dun, NNS), (say, VBP), (early, JJ), (hor, NN...  \n",
       "4  [(nah, RB), (think, NN), (goes, VBZ), (usf, JJ...  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms['pos_tags'] = sms['stopwords_removed'].apply(nltk.tag.pos_tag)\n",
    "sms.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1aP4xl6c_eB8"
   },
   "source": [
    "We are going to be using NLTK’s word lemmatizer which needs the parts of speech tags to be converted to wordnet’s format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "UE1a4riM_u2X",
    "outputId": "cef4a99c-e8bd-4cec-fa23-0cfeac6cafee"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\chara\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bQcreCGV_hxP"
   },
   "outputs": [],
   "source": [
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 717
    },
    "colab_type": "code",
    "id": "17-r9hWP_o0a",
    "outputId": "6759d3e4-7976-4b24-aa33-0e044eac4471"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\chara\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\omw-1.4.zip.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>msg</th>\n",
       "      <th>length</th>\n",
       "      <th>no_contract</th>\n",
       "      <th>msg_str</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>lower</th>\n",
       "      <th>no_punc</th>\n",
       "      <th>stopwords_removed</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>wordnet_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "      <td>[Go, until, jurong, point,, crazy.., Available...</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>[Go, until, jurong, point, ,, crazy, .., Avail...</td>\n",
       "      <td>[go, until, jurong, point, ,, crazy, .., avail...</td>\n",
       "      <td>[go, until, jurong, point, crazy, .., availabl...</td>\n",
       "      <td>[go, jurong, point, crazy, .., available, bugi...</td>\n",
       "      <td>[(go, VB), (jurong, JJ), (point, NN), (crazy, ...</td>\n",
       "      <td>[(go, v), (jurong, a), (point, n), (crazy, n),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>29</td>\n",
       "      <td>[Ok, lar..., Joking, wif, you, oni...]</td>\n",
       "      <td>Ok lar... Joking wif you oni...</td>\n",
       "      <td>[Ok, lar, ..., Joking, wif, you, oni, ...]</td>\n",
       "      <td>[ok, lar, ..., joking, wif, you, oni, ...]</td>\n",
       "      <td>[ok, lar, ..., joking, wif, you, oni, ...]</td>\n",
       "      <td>[ok, lar, ..., joking, wif, oni, ...]</td>\n",
       "      <td>[(ok, JJ), (lar, NN), (..., :), (joking, VBG),...</td>\n",
       "      <td>[(ok, a), (lar, n), (..., n), (joking, v), (wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>155</td>\n",
       "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, fin...</td>\n",
       "      <td>[(free, JJ), (entry, NN), (2, CD), (wkly, JJ),...</td>\n",
       "      <td>[(free, a), (entry, n), (2, n), (wkly, a), (co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>49</td>\n",
       "      <td>[YOU, dun, say, so, early, hor..., YOU, c, alr...</td>\n",
       "      <td>YOU dun say so early hor... YOU c already then...</td>\n",
       "      <td>[YOU, dun, say, so, early, hor, ..., YOU, c, a...</td>\n",
       "      <td>[you, dun, say, so, early, hor, ..., you, c, a...</td>\n",
       "      <td>[you, dun, say, so, early, hor, ..., you, c, a...</td>\n",
       "      <td>[dun, say, early, hor, ..., c, already, say, ...]</td>\n",
       "      <td>[(dun, NNS), (say, VBP), (early, JJ), (hor, NN...</td>\n",
       "      <td>[(dun, n), (say, v), (early, a), (hor, n), (.....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>61</td>\n",
       "      <td>[Nah, I, do not, think, he, goes, to, usf,, he...</td>\n",
       "      <td>Nah I do not think he goes to usf, he lives ar...</td>\n",
       "      <td>[Nah, I, do, not, think, he, goes, to, usf, ,,...</td>\n",
       "      <td>[nah, i, do, not, think, he, goes, to, usf, ,,...</td>\n",
       "      <td>[nah, i, do, not, think, he, goes, to, usf, he...</td>\n",
       "      <td>[nah, think, goes, usf, lives, around, though]</td>\n",
       "      <td>[(nah, RB), (think, NN), (goes, VBZ), (usf, JJ...</td>\n",
       "      <td>[(nah, r), (think, n), (goes, v), (usf, a), (l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                                msg  length  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...     111   \n",
       "1   ham                      Ok lar... Joking wif u oni...      29   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     155   \n",
       "3   ham  U dun say so early hor... U c already then say...      49   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...      61   \n",
       "\n",
       "                                         no_contract  \\\n",
       "0  [Go, until, jurong, point,, crazy.., Available...   \n",
       "1             [Ok, lar..., Joking, wif, you, oni...]   \n",
       "2  [Free, entry, in, 2, a, wkly, comp, to, win, F...   \n",
       "3  [YOU, dun, say, so, early, hor..., YOU, c, alr...   \n",
       "4  [Nah, I, do not, think, he, goes, to, usf,, he...   \n",
       "\n",
       "                                             msg_str  \\\n",
       "0  Go until jurong point, crazy.. Available only ...   \n",
       "1                    Ok lar... Joking wif you oni...   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3  YOU dun say so early hor... YOU c already then...   \n",
       "4  Nah I do not think he goes to usf, he lives ar...   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  [Go, until, jurong, point, ,, crazy, .., Avail...   \n",
       "1         [Ok, lar, ..., Joking, wif, you, oni, ...]   \n",
       "2  [Free, entry, in, 2, a, wkly, comp, to, win, F...   \n",
       "3  [YOU, dun, say, so, early, hor, ..., YOU, c, a...   \n",
       "4  [Nah, I, do, not, think, he, goes, to, usf, ,,...   \n",
       "\n",
       "                                               lower  \\\n",
       "0  [go, until, jurong, point, ,, crazy, .., avail...   \n",
       "1         [ok, lar, ..., joking, wif, you, oni, ...]   \n",
       "2  [free, entry, in, 2, a, wkly, comp, to, win, f...   \n",
       "3  [you, dun, say, so, early, hor, ..., you, c, a...   \n",
       "4  [nah, i, do, not, think, he, goes, to, usf, ,,...   \n",
       "\n",
       "                                             no_punc  \\\n",
       "0  [go, until, jurong, point, crazy, .., availabl...   \n",
       "1         [ok, lar, ..., joking, wif, you, oni, ...]   \n",
       "2  [free, entry, in, 2, a, wkly, comp, to, win, f...   \n",
       "3  [you, dun, say, so, early, hor, ..., you, c, a...   \n",
       "4  [nah, i, do, not, think, he, goes, to, usf, he...   \n",
       "\n",
       "                                   stopwords_removed  \\\n",
       "0  [go, jurong, point, crazy, .., available, bugi...   \n",
       "1              [ok, lar, ..., joking, wif, oni, ...]   \n",
       "2  [free, entry, 2, wkly, comp, win, fa, cup, fin...   \n",
       "3  [dun, say, early, hor, ..., c, already, say, ...]   \n",
       "4     [nah, think, goes, usf, lives, around, though]   \n",
       "\n",
       "                                            pos_tags  \\\n",
       "0  [(go, VB), (jurong, JJ), (point, NN), (crazy, ...   \n",
       "1  [(ok, JJ), (lar, NN), (..., :), (joking, VBG),...   \n",
       "2  [(free, JJ), (entry, NN), (2, CD), (wkly, JJ),...   \n",
       "3  [(dun, NNS), (say, VBP), (early, JJ), (hor, NN...   \n",
       "4  [(nah, RB), (think, NN), (goes, VBZ), (usf, JJ...   \n",
       "\n",
       "                                         wordnet_pos  \n",
       "0  [(go, v), (jurong, a), (point, n), (crazy, n),...  \n",
       "1  [(ok, a), (lar, n), (..., n), (joking, v), (wi...  \n",
       "2  [(free, a), (entry, n), (2, n), (wkly, a), (co...  \n",
       "3  [(dun, n), (say, v), (early, a), (hor, n), (.....  \n",
       "4  [(nah, r), (think, n), (goes, v), (usf, a), (l...  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('omw-1.4')\n",
    "sms['wordnet_pos'] = sms['pos_tags'].apply(lambda x: [(word, get_wordnet_pos(pos_tag)) for (word, pos_tag) in x])\n",
    "sms.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NCjEH6jG_iCE"
   },
   "source": [
    "Now we can apply NLTK’s word lemmatizer within our trusty list comprehension. Notice, the lemmatizer function requires two parameters the word and its tag (in wordnet form)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "96mw3ww8AGNx"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 717
    },
    "colab_type": "code",
    "id": "PJtg03P3_iXA",
    "outputId": "f8ade333-7ab6-4318-c133-c1d3dc548fe2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>msg</th>\n",
       "      <th>length</th>\n",
       "      <th>no_contract</th>\n",
       "      <th>msg_str</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>lower</th>\n",
       "      <th>no_punc</th>\n",
       "      <th>stopwords_removed</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>wordnet_pos</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "      <td>[Go, until, jurong, point,, crazy.., Available...</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>[Go, until, jurong, point, ,, crazy, .., Avail...</td>\n",
       "      <td>[go, until, jurong, point, ,, crazy, .., avail...</td>\n",
       "      <td>[go, until, jurong, point, crazy, .., availabl...</td>\n",
       "      <td>[go, jurong, point, crazy, .., available, bugi...</td>\n",
       "      <td>[(go, VB), (jurong, JJ), (point, NN), (crazy, ...</td>\n",
       "      <td>[(go, v), (jurong, a), (point, n), (crazy, n),...</td>\n",
       "      <td>go jurong point crazy .. available bugis n gre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>29</td>\n",
       "      <td>[Ok, lar..., Joking, wif, you, oni...]</td>\n",
       "      <td>Ok lar... Joking wif you oni...</td>\n",
       "      <td>[Ok, lar, ..., Joking, wif, you, oni, ...]</td>\n",
       "      <td>[ok, lar, ..., joking, wif, you, oni, ...]</td>\n",
       "      <td>[ok, lar, ..., joking, wif, you, oni, ...]</td>\n",
       "      <td>[ok, lar, ..., joking, wif, oni, ...]</td>\n",
       "      <td>[(ok, JJ), (lar, NN), (..., :), (joking, VBG),...</td>\n",
       "      <td>[(ok, a), (lar, n), (..., n), (joking, v), (wi...</td>\n",
       "      <td>ok lar ... joke wif oni ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>155</td>\n",
       "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, fin...</td>\n",
       "      <td>[(free, JJ), (entry, NN), (2, CD), (wkly, JJ),...</td>\n",
       "      <td>[(free, a), (entry, n), (2, n), (wkly, a), (co...</td>\n",
       "      <td>free entry 2 wkly comp win fa cup final tkts 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>49</td>\n",
       "      <td>[YOU, dun, say, so, early, hor..., YOU, c, alr...</td>\n",
       "      <td>YOU dun say so early hor... YOU c already then...</td>\n",
       "      <td>[YOU, dun, say, so, early, hor, ..., YOU, c, a...</td>\n",
       "      <td>[you, dun, say, so, early, hor, ..., you, c, a...</td>\n",
       "      <td>[you, dun, say, so, early, hor, ..., you, c, a...</td>\n",
       "      <td>[dun, say, early, hor, ..., c, already, say, ...]</td>\n",
       "      <td>[(dun, NNS), (say, VBP), (early, JJ), (hor, NN...</td>\n",
       "      <td>[(dun, n), (say, v), (early, a), (hor, n), (.....</td>\n",
       "      <td>dun say early hor ... c already say ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>61</td>\n",
       "      <td>[Nah, I, do not, think, he, goes, to, usf,, he...</td>\n",
       "      <td>Nah I do not think he goes to usf, he lives ar...</td>\n",
       "      <td>[Nah, I, do, not, think, he, goes, to, usf, ,,...</td>\n",
       "      <td>[nah, i, do, not, think, he, goes, to, usf, ,,...</td>\n",
       "      <td>[nah, i, do, not, think, he, goes, to, usf, he...</td>\n",
       "      <td>[nah, think, goes, usf, lives, around, though]</td>\n",
       "      <td>[(nah, RB), (think, NN), (goes, VBZ), (usf, JJ...</td>\n",
       "      <td>[(nah, r), (think, n), (goes, v), (usf, a), (l...</td>\n",
       "      <td>nah think go usf life around though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                                msg  length  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...     111   \n",
       "1   ham                      Ok lar... Joking wif u oni...      29   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     155   \n",
       "3   ham  U dun say so early hor... U c already then say...      49   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...      61   \n",
       "\n",
       "                                         no_contract  \\\n",
       "0  [Go, until, jurong, point,, crazy.., Available...   \n",
       "1             [Ok, lar..., Joking, wif, you, oni...]   \n",
       "2  [Free, entry, in, 2, a, wkly, comp, to, win, F...   \n",
       "3  [YOU, dun, say, so, early, hor..., YOU, c, alr...   \n",
       "4  [Nah, I, do not, think, he, goes, to, usf,, he...   \n",
       "\n",
       "                                             msg_str  \\\n",
       "0  Go until jurong point, crazy.. Available only ...   \n",
       "1                    Ok lar... Joking wif you oni...   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3  YOU dun say so early hor... YOU c already then...   \n",
       "4  Nah I do not think he goes to usf, he lives ar...   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  [Go, until, jurong, point, ,, crazy, .., Avail...   \n",
       "1         [Ok, lar, ..., Joking, wif, you, oni, ...]   \n",
       "2  [Free, entry, in, 2, a, wkly, comp, to, win, F...   \n",
       "3  [YOU, dun, say, so, early, hor, ..., YOU, c, a...   \n",
       "4  [Nah, I, do, not, think, he, goes, to, usf, ,,...   \n",
       "\n",
       "                                               lower  \\\n",
       "0  [go, until, jurong, point, ,, crazy, .., avail...   \n",
       "1         [ok, lar, ..., joking, wif, you, oni, ...]   \n",
       "2  [free, entry, in, 2, a, wkly, comp, to, win, f...   \n",
       "3  [you, dun, say, so, early, hor, ..., you, c, a...   \n",
       "4  [nah, i, do, not, think, he, goes, to, usf, ,,...   \n",
       "\n",
       "                                             no_punc  \\\n",
       "0  [go, until, jurong, point, crazy, .., availabl...   \n",
       "1         [ok, lar, ..., joking, wif, you, oni, ...]   \n",
       "2  [free, entry, in, 2, a, wkly, comp, to, win, f...   \n",
       "3  [you, dun, say, so, early, hor, ..., you, c, a...   \n",
       "4  [nah, i, do, not, think, he, goes, to, usf, he...   \n",
       "\n",
       "                                   stopwords_removed  \\\n",
       "0  [go, jurong, point, crazy, .., available, bugi...   \n",
       "1              [ok, lar, ..., joking, wif, oni, ...]   \n",
       "2  [free, entry, 2, wkly, comp, win, fa, cup, fin...   \n",
       "3  [dun, say, early, hor, ..., c, already, say, ...]   \n",
       "4     [nah, think, goes, usf, lives, around, though]   \n",
       "\n",
       "                                            pos_tags  \\\n",
       "0  [(go, VB), (jurong, JJ), (point, NN), (crazy, ...   \n",
       "1  [(ok, JJ), (lar, NN), (..., :), (joking, VBG),...   \n",
       "2  [(free, JJ), (entry, NN), (2, CD), (wkly, JJ),...   \n",
       "3  [(dun, NNS), (say, VBP), (early, JJ), (hor, NN...   \n",
       "4  [(nah, RB), (think, NN), (goes, VBZ), (usf, JJ...   \n",
       "\n",
       "                                         wordnet_pos  \\\n",
       "0  [(go, v), (jurong, a), (point, n), (crazy, n),...   \n",
       "1  [(ok, a), (lar, n), (..., n), (joking, v), (wi...   \n",
       "2  [(free, a), (entry, n), (2, n), (wkly, a), (co...   \n",
       "3  [(dun, n), (say, v), (early, a), (hor, n), (.....   \n",
       "4  [(nah, r), (think, n), (goes, v), (usf, a), (l...   \n",
       "\n",
       "                                          lemmatized  \n",
       "0  go jurong point crazy .. available bugis n gre...  \n",
       "1                        ok lar ... joke wif oni ...  \n",
       "2  free entry 2 wkly comp win fa cup final tkts 2...  \n",
       "3            dun say early hor ... c already say ...  \n",
       "4                nah think go usf life around though  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "sms['lemmatized'] = sms['wordnet_pos'].apply(lambda x: \" \".join([wnl.lemmatize(word, tag) for word, tag in x]))\n",
    "sms.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vpqlSX_JASXz"
   },
   "source": [
    "Lastly, we should save all of our pre-processing work for the next steps in the workflow. We can simnple save it as a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PRKemvFgAc4S"
   },
   "outputs": [],
   "source": [
    "sms.to_csv('sms_spam_collection.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the TF-IDF Vertorizer to convert the raw documents into feature matrix which is very important to train a Machine Learning model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "porter=PorterStemmer()\n",
    "def tokenizer(text):\n",
    "        return text.split()\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None,lowercase=False,preprocessor=None,tokenizer=tokenizer_porter,use_idf=True,norm='l2',smooth_idf=True)\n",
    "y = sms.label.values\n",
    "x=tfidf.fit_transform(sms.lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Machine Learning Model\n",
    "Now to train a machine learning model I will split the data into 50 percent training and 50 percent test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(x,y,random_state=1,test_size=0.5,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s train a machine learning model for the task of sentiment analysis by using the Logistic Regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "clf=LogisticRegressionCV(cv=6,scoring='accuracy',random_state=0,n_jobs=-1,verbose=3,max_iter=500).fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   6 | elapsed:   15.5s remaining:   31.2s\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:   23.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9723618090452262\n"
     ]
    }
   ],
   "source": [
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K3_uGCKBXV6z"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Intro to NLP: Week 1 - NLP 101.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
